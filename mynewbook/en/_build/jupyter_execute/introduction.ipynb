{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46677ff",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "## Motivation\n",
    "\n",
    "Since its creation in 2001, Wikipedia has become one of the most epistemic reliable digital platform in the world [Fallis, 2008](https://doi.org/10.1002/asi.20870), attracting almost 2 billions of monthly readers in 2025 seeking information on current and historical events [Fallis, 2008](https://doi.org/10.1002/asi.20870). Its open “wiki” model allowing anyone to edit, constitutes both its greatest strength and its main vulnerability [Kurek et al., 2023]. On the one hand, a vast community of daily editors contributes to the expansion of global human knowledge. On the other hand, Wikipedia is also exposed to large-scale campaigns of misinformation [Saez-Trumper, 2019](https://arxiv.org/abs/1910.12596), manipulation [Makovska, 2025](https://hdl.handle.net/20.500.14570/5697) and more recently, the weaponization of cultural heritage. This study investigates how Wikipedia can function as a platform for “weaponization” edits, understood as the strategic manipulation, distortion, or erasure of cultural identity, memory, and heritage for political or military purposes. \n",
    "\n",
    "## Goal\n",
    "\n",
    "The purpose of this project is to have a proper dataset that can be used in the future for training models that detect weaponized content in the context of the [2026 CROSS project](https://www.epfl.ch/schools/cdh/cross-2026/). This requires comprehensive preprocessing and deep examination of the dataset. This report outlines the methods used to preprocess and analyse the data, with the aim of enhancing our understanding of its structure and revealing meaningful patterns. Combined with Hidi's project, we aimed to answer four questions : \n",
    "\n",
    "::::{grid} 1 1 2 3\n",
    "\n",
    ":::{card}\n",
    ":header: What ?\n",
    "Which types of content or topics related to cultural heritage are primarily targeted?\n",
    ":::\n",
    "\n",
    ":::{card}\n",
    ":header: How ?\n",
    "Which techniques—semantic, structural, or algorithmic—are employed by actors who engage in the weaponization of content?\n",
    ":::\n",
    "\n",
    ":::{card}\n",
    ":header: Who ?\n",
    "Who are the users responsible for weaponizing content, and what are their main characteristics?\n",
    ":::\n",
    "\n",
    ":::{card}\n",
    ":header: Why ?\n",
    "What motivations drive users to manipulate content related to cultural heritage?\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "Previous phases of the research project has already partially addressed the *What* and *How* questions. Consequently, this project initially focused on addressing the *Who* question; however, as the investigation progressed, it also revealed new insights into the types of content being targeted and the techniques used to manipulate it.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "Joining an ongoing project presents several methodological challenges. It first requires developing a clear understanding of the work already completed, while simultaneously handling and restructuring large, unfamiliar datasets and identifying the expected analytical outcomes necessary to move the project forward. Throughout this study, working with Wikipedia edits and metadata proved to be particularly demanding. Although the [Wikimedia Action API](https://www.mediawiki.org/wiki/API:Action_API) provides extensive access to information about users, edits, and articles, its documentation is often incomplete or insufficiently detailed. As a result, additional effort and time were required to identify the appropriate endpoints and parameters needed for specific analyses.\n",
    "\n",
    "A major and recurrent challenge across nearly all feature analyses concerns the limited availability of temporal metadata. For many features of interest, such as user group membership, article quality assessments, or protection status, information is generally accessible only in its current state, rather than as it existed at the time an edit was made. Building a reliable dataset therefore requires reconstructing historical contexts that are often missing, inconsistently recorded, or buried within extensive Wikipedia logs.\n",
    "\n",
    "Finally, working with raw Wikipedia text and data from raw collected text introduces substantial preprocessing complexity. Article content and edit comments require extensive cleaning and normalisation using custom processing."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}