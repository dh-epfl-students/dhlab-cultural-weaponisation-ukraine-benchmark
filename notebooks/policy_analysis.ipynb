{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0486320e-abc2-4aa4-9662-0a8dc7a50316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e9a4547-9cad-4196-a01d-c089219bc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Articles List\n",
    "control_articles = [\n",
    "    \"Pop music\",\n",
    "    \"Rock and roll\",\n",
    "    \"Eric Clapton\",\n",
    "    \"Rolling Stone\",\n",
    "    \"Jazz\",\n",
    "    \"Swing\",\n",
    "    \"Classical music\",\n",
    "    \"Ludwig van Beethoven\",\n",
    "    \"Wolfgang Amadeus Mozart\",\n",
    "    \"Joseph Haydn\",\n",
    "    \"Country music\",\n",
    "    \"BTS (groupe)\",\n",
    "    \"K-Pop\",\n",
    "    \"Electronic music\",\n",
    "    \"Daft Punk\",\n",
    "    \"Paul Kalkbrenner\",\n",
    "    \"Trumpet\",\n",
    "    \"Music theory\",\n",
    "    \"Fender\",\n",
    "    \"Marshall Amplification\",\n",
    "    \"Jimi Hendrix\",\n",
    "    \"Bob Marley\",\n",
    "    \"Edith Piaf\",\n",
    "    \"Royal Albert Hall\",\n",
    "    \"Piano\",\n",
    "    \"Saxophone\",\n",
    "    \"Pink Floyd\",\n",
    "    \"Nirvana (band)\",\n",
    "    \"Nina Simone\",\n",
    "    \"Music of Africa\",\n",
    "    \"Major scale\",\n",
    "    \"Major chord\",\n",
    "    \"Minor chord\",\n",
    "    \"AC/DC\",\n",
    "    \"Red Hot Chili Peppers\",\n",
    "    \"Funk rock\",\n",
    "    \"James Brown\",\n",
    "    \"Dire Straits\",\n",
    "    \"Mark Knofler\",\n",
    "    \"John Frusciante\",\n",
    "    \"Alan Clark\",\n",
    "    \"Bob Dylan\",\n",
    "    \"The Beatles\",\n",
    "    \"Stevie Wonder\",\n",
    "    \"Guitar\"\n",
    "]\n",
    "\n",
    "articles = [\n",
    "    \"COVID-19 pandemic in Ukraine\",\n",
    "    \"History of Ukraine\",\n",
    "    \"Crimea\",\n",
    "    \"Russian annexation of Crimea\",\n",
    "    \"2004 Ukrainian presidential election\",\n",
    "    \"Football in Ukraine\",\n",
    "    \"Bessarabia\",\n",
    "    \"2014 pro-Russian unrest in Ukraine\",\n",
    "    \"Communist Party of the Soviet Union\",\n",
    "    \"English Civil War\",\n",
    "    \"Christianity in Russia\",\n",
    "    \"History of Christianity in Ukraine\",\n",
    "    \"Flag of Ukraine\",\n",
    "    \"Alexander II of Russia\",\n",
    "    \"Eastern Front (World War II)\",\n",
    "    \"Bukovina\",\n",
    "    \"Epiphanius I of Ukraine\",\n",
    "    \"History of Crimea\",\n",
    "    \"Dissolution of the Soviet Union\",\n",
    "    \"Crimean Tatars\",\n",
    "    \"Catherine the Great\",\n",
    "    \"Culture of Ukraine\",\n",
    "    \"Abortion in Ukraine\",\n",
    "    \"Christmas in Ukraine\",\n",
    "    \"Armed Forces of Ukraine\",\n",
    "    \"Demographics of Ukraine\",\n",
    "    \"History of Kyiv\",\n",
    "    \"Foreign relations of Ukraine\",\n",
    "    \"Eastern Front (World War I)\",\n",
    "    \"Economy of Ukraine\",\n",
    "    \"Galicia (Eastern Europe)\",\n",
    "    \"Euromaidan\",\n",
    "    \"History of the Russian Orthodox Church\",\n",
    "    \"Government of Ukraine\",\n",
    "    \"Geography of Ukraine\",\n",
    "    \"Censuses in Ukraine\",\n",
    "    \"Administrative divisions of Ukraine\",\n",
    "    \"Government of the Ukrainian People's Republic in exile\",\n",
    "    \"Education in Ukraine\",\n",
    "    \"2022 Russian invasion of Ukraine\",\n",
    "    \"Buddhism in Ukraine\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ffae441-ff4f-4b59-b3e0-4174e8c59d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved at: ../datas/interim/Policy Analysis/policy_analysis_articles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import mwparserfromhell\n",
    "import re\n",
    "\n",
    "API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"DH_Project/1.0 (maxime.garambois@epfl.ch)\"\n",
    "}\n",
    "\n",
    "CONTENTIOUS_KEYWORDS = [\"contentious topics/\"]\n",
    "\n",
    "def get_protection_status(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": \"title\",\n",
    "        \"prop\": \"info\",\n",
    "        \"inprop\": \"protection\",\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\" : \"2\"\n",
    "    }\n",
    "\n",
    "    RES = requests.get(API, params=params, headers=HEADERS)\n",
    "    DATA = RES.json()\n",
    "\n",
    "    page = data[\"query\"][\"pages\"][0]\n",
    "\n",
    "    if \"missing\" in page:\n",
    "        return {\"error\": \"Page does not exist\"}\n",
    "\n",
    "    protection = page.get(\"protection\", [])\n",
    "\n",
    "    # Clean up into simpler dict form\n",
    "    protection_clean = [\n",
    "        {\n",
    "            \"type\": p[\"type\"],\n",
    "            \"level\": p[\"level\"],\n",
    "            \"expiry\": p[\"expiry\"]\n",
    "        }\n",
    "        for p in protection\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"title\": page[\"title\"],\n",
    "        \"protection\": protection_clean\n",
    "    }\n",
    "\n",
    "def get_talk_wikitext(title):\n",
    "    \"\"\"Retrieve raw wikitext of the Talk page.\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": f\"Talk:{title}\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(API, params=params, headers=HEADERS)\n",
    "    data = response.json()\n",
    "\n",
    "    page = data[\"query\"][\"pages\"][0]\n",
    "    if \"missing\" in page:\n",
    "        return None  # talk page doesn't exist\n",
    "\n",
    "    return page[\"revisions\"][0][\"slots\"][\"main\"][\"content\"]\n",
    "\n",
    "\n",
    "def parse_assessments(wikitext):\n",
    "    \"\"\"Parse class, importance values, and contentious-topic status from wikitext.\"\"\"\n",
    "    code = mwparserfromhell.parse(wikitext)\n",
    "\n",
    "    results = {\n",
    "        \"class\": None,\n",
    "        \"importance\": {},\n",
    "        \"contentious\": False\n",
    "    }\n",
    "\n",
    "    for template in code.filter_templates():\n",
    "        name = template.name.strip().lower()\n",
    "\n",
    "        # GLOBAL CLASS (from banner shell)\n",
    "        if \"banner shell\" in name:\n",
    "            if template.has(\"class\"):\n",
    "                results[\"class\"] = str(template.get(\"class\").value).strip()\n",
    "\n",
    "        # PER-WIKIPROJECT IMPORTANCE\n",
    "        if \"wikiproject\" in name and not \"banner shell\" in name:\n",
    "            project = template.name.strip().replace(\"WikiProject\", \"\").strip()\n",
    "\n",
    "            # look for either \"importance\" or \"priority\"\n",
    "            if template.has(\"importance\"):\n",
    "                imp = str(template.get(\"importance\").value).strip()\n",
    "                results[\"importance\"][project] = imp\n",
    "            elif template.has(\"priority\"):\n",
    "                # Some projects use \"priority\" (e.g., Mathematics)\n",
    "                imp = str(template.get(\"priority\").value).strip()\n",
    "                results[\"importance\"][project] = imp\n",
    "\n",
    "        # CONTENTIOUS TOPICS DETECTION\n",
    "        temp_text = str(template).lower()\n",
    "        if any(keyword in name for keyword in CONTENTIOUS_KEYWORDS):\n",
    "            results[\"contentious\"] = True\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_article_assessment(title):\n",
    "    \"\"\"Main wrapper: fetch talk page and parse assessment.\"\"\"\n",
    "    wikitext = get_talk_wikitext(title)\n",
    "    if not wikitext:\n",
    "        return {\"error\": \"Talk page does not exist\"}\n",
    "\n",
    "    return parse_assessments(wikitext)\n",
    "\n",
    "def extract_relevant_importance(importance_dict):\n",
    "    \"\"\"\n",
    "    From all WikiProject importance values:\n",
    "    - If 'Ukraine' exists -> return that value.\n",
    "    - Else -> return the first value in the dict.\n",
    "    - If dict empty -> return None.\n",
    "    \"\"\"\n",
    "    if not importance_dict:\n",
    "        return None\n",
    "\n",
    "    # Prefer Ukraine rating if present\n",
    "    if \"Ukraine\" in importance_dict:\n",
    "        return importance_dict[\"Ukraine\"]\n",
    "\n",
    "    # Otherwise take the first key in the dict\n",
    "    first_key = next(iter(importance_dict))\n",
    "    return importance_dict[first_key]\n",
    "\n",
    "def get_data(articles_list, out_csv):\n",
    "    rows = []\n",
    "\n",
    "    for article in articles_list:\n",
    "        assessment = get_article_assessment(article)\n",
    "\n",
    "        # Extract global class\n",
    "        article_class = assessment.get(\"class\")\n",
    "\n",
    "        # Extract importance from rules\n",
    "        importance = extract_relevant_importance(assessment.get(\"importance\", {}))\n",
    "\n",
    "        # Contentious topic boolean\n",
    "        contentious = assessment.get(\"contentious\")\n",
    "\n",
    "        rows.append({\n",
    "            \"article\": article,\n",
    "            \"class\": article_class,\n",
    "            \"importance\": importance,\n",
    "            \"contentious\": contentious\n",
    "        })\n",
    "\n",
    "    # Save CSV\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"article\", \"class\", \"importance\", \"contentious\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"✅ CSV saved at: {out_csv}\")\n",
    "\n",
    "OUT_CSV = '../datas/interim/Policy Analysis/policy_analysis_articles.csv'\n",
    "get_data(articles, OUT_CSV)\n",
    "status = []\n",
    "for article in articles:\n",
    "    d = get_protection_status(article)\n",
    "    status.append(d[\"protection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "959b2b17-edbd-4131-aa14-5171aff1a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19 pandemic in Ukraine None\n",
      "History of Ukraine None\n",
      "Crimea None\n",
      "Annexation of Crimea by the Russian Federation None\n",
      "2004 Ukrainian presidential election None\n",
      "Football in Ukraine None\n",
      "Bessarabia None\n",
      "2014 pro-Russian unrest in Ukraine None\n",
      "Communist Party of the Soviet Union None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     level = \u001b[43mget_vital_level_by_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mprint\u001b[39m(article, level)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mget_vital_level_by_list\u001b[39m\u001b[34m(article_title)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m6\u001b[39m):\n\u001b[32m     17\u001b[39m     params = {\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maction\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mparse\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWikipedia:Vital_articles/Level/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprop\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     res = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m)\u001b[49m.json()\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/urllib3/connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    752\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    755\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/ada/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "import re\n",
    "\n",
    "API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\"User-Agent\": \"DH_Project/1.0 (maxime.garambois@epfl.ch)\"}\n",
    "\n",
    "def mw_normalize_and_redirects(title):\n",
    "    \"\"\"Return canonical title + any redirects (all with underscores).\"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"redirects\": \"1\",\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "    }\n",
    "    r = requests.get(API, params=params, headers=HEADERS).json()\n",
    "    pages = r.get(\"query\", {}).get(\"pages\", [])\n",
    "    if not pages or \"missing\" in pages[0]:\n",
    "        # fall back to the provided title\n",
    "        return {title.replace(\" \", \"_\")}\n",
    "    canonical = pages[0][\"title\"].replace(\" \", \"_\")\n",
    "    candidates = {canonical}\n",
    "    for redir in r[\"query\"].get(\"redirects\", []):\n",
    "        candidates.add(redir[\"from\"].replace(\" \", \"_\"))\n",
    "        candidates.add(redir[\"to\"].replace(\" \", \"_\"))\n",
    "    return candidates\n",
    "\n",
    "def parse_page_html(title):\n",
    "    \"\"\"Fetch parsed HTML for a wiki page title.\"\"\"\n",
    "    params = {\"action\": \"parse\", \"page\": title, \"prop\": \"text\", \"format\": \"json\"}\n",
    "    r = requests.get(API, params=params, headers=HEADERS).json()\n",
    "    if \"error\" in r:\n",
    "        return None\n",
    "    return r[\"parse\"][\"text\"][\"*\"]\n",
    "\n",
    "def collect_level_subpages(level):\n",
    "    \"\"\"\n",
    "    From the root VA page for a level, collect all subpages like:\n",
    "    Wikipedia:Vital articles/Level/<level>/People, /History, etc.\n",
    "    Include the root too (some levels have direct links).\n",
    "    \"\"\"\n",
    "    root = f\"Wikipedia:Vital articles/Level/{level}\"\n",
    "    html = parse_page_html(root)\n",
    "    subpages = set()\n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\"):\n",
    "            href = a.get(\"href\", \"\")\n",
    "            title = a.get(\"title\", \"\")\n",
    "            # Prefer title (cleaner), but fall back to href if needed\n",
    "            if title.startswith(f\"Wikipedia:Vital articles/Level/{level}/\"):\n",
    "                subpages.add(title)\n",
    "            elif href.startswith(\"/wiki/Wikipedia:Vital_articles/Level/\"):\n",
    "                # Extract after /wiki/\n",
    "                target = href[len(\"/wiki/\"):]\n",
    "                if re.match(rf\"Wikipedia:Vital_articles/Level/{level}\\b\", target):\n",
    "                    subpages.add(target)\n",
    "    subpages.add(root)\n",
    "    return subpages\n",
    "\n",
    "def vital_level_via_lists(article_title):\n",
    "    \"\"\"\n",
    "    Search Vital Articles lists (levels 1..5) and return the level number\n",
    "    where the article appears, or None if not found.\n",
    "    \"\"\"\n",
    "    acceptable = {t.lower() for t in mw_normalize_and_redirects(article_title)}\n",
    "\n",
    "    for level in range(1, 6):\n",
    "        for subpage in collect_level_subpages(level):\n",
    "            html = parse_page_html(subpage)\n",
    "            if not html:\n",
    "                continue\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            for a in soup.find_all(\"a\"):\n",
    "                # Use the title attribute: it's the canonical page title\n",
    "                if a.has_attr(\"title\"):\n",
    "                    link_title = a[\"title\"].replace(\" \", \"_\").lower()\n",
    "                    if link_title in acceptable:\n",
    "                        return level\n",
    "                else:\n",
    "                    # Fallback to href if no title (rare)\n",
    "                    href = a.get(\"href\", \"\")\n",
    "                    if href.startswith(\"/wiki/\"):\n",
    "                        target = href[len(\"/wiki/\"):].split(\"#\", 1)[0]\n",
    "                        target = unquote(target).replace(\" \", \"_\").lower()\n",
    "                        if target in acceptable:\n",
    "                            return level\n",
    "    return None\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    level = get_vital_level_by_list(article)\n",
    "    print(article, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "13a6975b-b351-4d91-84cb-da5f0a116495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datas/interim/Policy Analysis/policy_analysis_articles.csv')\n",
    "df.loc[26, 'contentious'] = True\n",
    "df.loc[26, 'importance'] = 'High'\n",
    "df.loc[16, 'importance'] = None\n",
    "df.loc[37, 'importance'] = None\n",
    "df.loc[37, 'contentious'] = False\n",
    "df.loc[37, 'class'] = 'Start'\n",
    "df.loc[40, 'importance'] = None\n",
    "df['protection'] = status\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b467754-073a-4237-af95-dfa99d6d8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../datas/interim/Policy Analysis/policy_analysis_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f9ca29d3-28e5-41ef-b4ce-ad81aa1e760f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Talk:Albert Einstein', 'protection': []}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"DH_Project/1.0 (maxime.garambois@epfl.ch)\"\n",
    "}\n",
    "\n",
    "CONTENTIOUS_KEYWORDS = [\"contentious topics/\"]\n",
    "\n",
    "def get_protection_status(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"info\",\n",
    "        \"inprop\": \"protection\",\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\" : \"2\"\n",
    "    }\n",
    "\n",
    "    RES = requests.get(API, params=params, headers=HEADERS)\n",
    "    DATA = RES.json()\n",
    "\n",
    "    page = DATA[\"query\"][\"pages\"][0]\n",
    "\n",
    "    if \"missing\" in page:\n",
    "        return {\"error\": \"Page does not exist\"}\n",
    "\n",
    "    protection = page.get(\"protection\", [])\n",
    "\n",
    "    # Clean up into simpler dict form\n",
    "    protection_clean = [\n",
    "        {\n",
    "            \"type\": p[\"type\"],\n",
    "            \"level\": p[\"level\"],\n",
    "            \"expiry\": p[\"expiry\"]\n",
    "        }\n",
    "        for p in protection\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"title\": page[\"title\"],\n",
    "        \"protection\": protection_clean\n",
    "    }\n",
    "\n",
    "title = 'Albert Einstein'\n",
    "get_protection_status(title)\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADA Env",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
